
# ðŸ§  Document-Aware Conversational Assistant  
### A RAG-Based NLP System  


## ðŸ“˜ Abstract  
This project implements a **Document-Aware Conversational Assistant** using **Retrieval-Augmented Generation (RAG)**. Users can upload PDF documents and ask questions naturally in English. The system retrieves contextually relevant sections using **semantic embeddings** and generates grounded responses using a **large language model (LLM)**.  

It demonstrates core NLP techniques such as **text preprocessing**, **sentence embeddings**, **cosine similarity-based retrieval**, and **context-grounded response generation**.  

***

## â“ Problem Statement  
Finding specific information in long documents is challenging.  
Traditional keyword search often fails because of **vocabulary mismatch** â€” for example, searching *â€œvacation policyâ€* may not return results from a section labeled *â€œannual leaveâ€*.  

This project solves that with **semantic search**, which understands meaning instead of relying on exact keyword matches.  

***

## ðŸ’¡ Solution: RAG Architecture  
**RAG = Retrieval-Augmented Generation**

Instead of letting the LLM â€œguessâ€ from memory (causing hallucination), this system:  
1. Retrieves relevant chunks from uploaded documents.  
2. Augments the user prompt with retrieved context.  
3. Generates answers grounded in actual sources.  

This improves accuracy, reduces hallucination, and enables **source transparency**.  

***

## ðŸ—‚ï¸ Project Structure  

```bash
document-assistant/
â”œâ”€â”€ config.py           # Configuration constants
â”œâ”€â”€ utils.py            # Helper functions
â”œâ”€â”€ nlp_core.py         # Core NLP logic
â”œâ”€â”€ app.py              # Streamlit UI
â”œâ”€â”€ requirements.txt    # Dependencies
â”œâ”€â”€ .env                # API keys (excluded from git)
â””â”€â”€ README.md           # Documentation
```

### File Overview
| File | Purpose | Key Contents |
|------|----------|--------------|
| `config.py` | Configuration settings | API keys, model names, chunk size (800), overlap (200), top-k (4), temperature (0.7) |
| `utils.py` | Helper functions | `truncate_text()`, `format_percentage()`, `parse_key_value_string()`, `safe_get()` |
| `nlp_core.py` | NLP logic | Text extraction, cleaning, chunking, embedding, similarity search, RAG pipeline |
| `app.py` | Streamlit UI | Chat interface with tabs (chat, infographic, image prompt) |
| `requirements.txt` | Python dependencies | Streamlit, Sentence Transformers, PyMuPDF, NumPy, OpenAI |
| `.env` | Secrets | `OPENAI_API_KEY=sk-xxxxx` (excluded from Git) |

***

## ðŸ—ï¸ System Architecture  

### **Indexing Phase (Document Upload)**  
PDF â†’ Extract (PyMuPDF) â†’ Clean â†’ Chunk (800c/200o) â†’ Embed (384-dim) â†’ Store in RAM  

### **Query Phase (User Question)**  
Question â†’ Embed â†’ Cosine Similarity â†’ Top-4 Chunks â†’ Build Prompt â†’ GPT-4o-mini â†’ Answer + Sources  

***

## âš™ï¸ Key Implementation Details  

### **6.1 Text Processing**  
- Extraction: PyMuPDF extracts text from PDFs.  
- Cleaning: Removes noise such as extra spaces and special characters.  
- Chunking: 800-character chunks with 200-character overlap, aligned on sentence boundaries.  

### **6.2 Embeddings & Search**  
- Model: `all-MiniLM-L6-v2`  
- Vector Size: 384 dimensions per chunk.  
- Search: Cosine similarity for semantic closeness.  
- Retrieval: Top 4 most relevant chunks returned.  

### **6.3 Answer Generation**  
- LLM: OpenAI `gpt-4o-mini`.  
- Prompt: Retrieved chunks + last 3 conversation turns + current question.  
- Constraint: Answers are restricted to document context.  

### **6.4 Memory Solution**  
- Memory storage: `st.session_state.chat_history`.  
- Injects last 3 conversation turns into each prompt for continuity.  

### **6.5 Infographic Generation**  
- Extracts structured data from LLM responses â†’ Parses into dictionary â†’ Injects into HTML template.  
- HTML allows fast, local visualization without external APIs.  

***

## ðŸ§© Technologies Used  

| Component | Technology | Why Chosen |
|------------|-------------|-----------|
| UI | Streamlit | Easy Python-native UI |
| PDF Extraction | PyMuPDF | Fast and stable |
| Embeddings | Sentence-Transformers | Compact & high-quality |
| LLM | OpenAI GPT-4o-mini | Cost-efficient and reliable |
| Vector Storage | NumPy | Simple for demo-scale needs |
| Similarity | Cosine Similarity | Captures semantic meaning |

**Alternatives Not Used:**  
- **React/Flask:** Added complexity for prototype.  
- **Word2Vec:** Word-level only, lacks sentence context.  
- **Local LLM:** Requires GPU.  
- **FAISS/Pinecone:** Overkill for small demos.  

***

## ðŸŒŸ Features  

- ðŸ“„ PDF Upload & Text Extraction  
- ðŸ§  Natural-Language Search & Q&A  
- ðŸ” Source Attribution (view retrieved chunks)  
- ðŸ’¬ Conversation Memory for follow-ups  
- ðŸ—ƒï¸ Multi-document Support  
- ðŸ“Š HTML Infographic Generation  
- ðŸŽ¨ AI Image Prompt Generator  

***

## ðŸ”§ Key Parameters  

| Parameter | Value | Reason |
|------------|--------|--------|
| Chunk Size | 800 chars | Keeps paragraph context |
| Overlap | 200 chars | Avoids boundary loss |
| Top-k | 4 | Covers context while avoiding noise |
| Embedding Dims | 384 | From MiniLM model |
| Memory Turns | 3 | Retains limited chat continuity |
| Temperature | 0.7 | Balances factual and creative tone |

***

## âš ï¸ Limitations  

| Limitation | Reason | Future Fix |
|-------------|---------|------------|
| Data lost on refresh | Stored in memory only | Add persistent database |
| Needs internet | OpenAI API dependency | Use local LLM |
| Text-only | No image/table handling | Multi-modal RAG |
| Small scale | NumPy brute-force search | Upgrade to FAISS/Pinecone |
| Noisy extraction | Complex PDF layouts | Enhanced parsing pipeline |

***

## ðŸš€ Future Scope  
- Integrate **FAISS/Pinecone** for scalable vector storage.  
- Add **local LLMs** (Llama/Mistral) for offline use.  
- Support **multi-modal RAG** (text + tables + images).  
- Introduce **hybrid retrieval** (semantic + keyword).  
- Implement **user authentication & document libraries**.  

***

## ðŸ§­ How to Run  

```bash
# 1. Install dependencies
pip install -r requirements.txt

# 2. Create a .env file
echo "OPENAI_API_KEY=sk-your-api-key-here" > .env

# 3. Launch the app
streamlit run app.py
```

***

## ðŸ Conclusion  
The **Document-Aware Conversational Assistant** demonstrates how **RAG** can blend **semantic search** with **language generation** to build intelligent, grounded NLP systems.  
It mitigates vocabulary mismatch, reduces hallucination, and forms the foundation for scalable tools like **ChatGPT (file upload)** and **Google NotebookLM**.  

***

## ðŸ“š References  
1. Lewis, P. et al. (2020). *Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.* NeurIPS.  
2. Reimers, N., & Gurevych, I. (2019). *Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks.*  
3. [Sentence-Transformers Documentation](https://www.sbert.net/)  
4. [Streamlit Docs](https://docs.streamlit.io/)  
5. [OpenAI API Docs](https://platform.openai.com/docs/)  

***

## ðŸ“¦ Appendix  

### **requirements.txt**
```
streamlit>=1.28.0
sentence-transformers>=2.2.0
pymupdf>=1.23.0
numpy>=1.24.0
openai>=1.0.0
python-dotenv>=1.0.0
```

### **.env (sample)**
```
OPENAI_API_KEY=sk-your-api-key-here
```

### **.gitignore**
```
.env
__pycache__/
*.pyc
.streamlit/
```

**Total Files:** 6 (`config.py`, `utils.py`, `nlp_core.py`, `app.py`, `requirements.txt`, `.env`)  
**Total LOC:** ~600  
**Core NLP Techniques:** Text preprocessing -  Sentence Embeddings -  Cosine Similarity -  RAG Pipeline -  Prompt Engineering  
